{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-tuning Notebook\n",
    "The purpose of this notebook is exploring and fine-tuning the short listed candidate models to get our best model. In the flow, this notebook covers from step 12 and on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for imports of .py modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd())\n",
    "path = str(path)\n",
    "print(path)\n",
    "sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.sml_utils as sml_utils\n",
    "import utils.regression_utils as reg_utils\n",
    "import utils.all_attr_eda_utils as aae_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = 'data/winequality-white.csv'\n",
    "target_attr = 'quality'\n",
    "test_size = 0.20\n",
    "train_test_split_random_state = 42\n",
    "missingness_threshold = 0.20\n",
    "\n",
    "# step 9\n",
    "elastic_net_random_state = 42\n",
    "decision_tree_random_state = 42\n",
    "random_forest_random_state = 42\n",
    "gradient_boosting_random_state = 42\n",
    "target_encoder_random_state = 42\n",
    "\n",
    "# step 10\n",
    "scoring = ['neg_root_mean_squared_error', 'neg_mean_absolute_error', 'max_error', 'r2']\n",
    "kwargs = {'return_indices': False}  # if true the indices of the cross validation split are returned\n",
    "max_iter = 10000  # max number of iterations for Lasso, Ridge and ElasticNet - default was 1000\n",
    "kfold_n_splits = 10  # number of folds in k-fold cv\n",
    "kfold_shuffle = True\n",
    "kfold_random_state = 42\n",
    "\n",
    "# step 12\n",
    "gs_cv_kfold_n_splits = 10\n",
    "gs_cv_kfold_shuffle = True\n",
    "gs_cv_kfold_random_state = 24\n",
    "show_all_params = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset variables (train/test dataframes were created in previous phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/wine_train_df.csv').copy() # Make copy so original is not affected\n",
    "train_cap_x_df = train_df.iloc[:, :-1]  # All columns except the last one\n",
    "train_y_df = train_df.iloc[:, -1].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking missingness (also done in previous phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna(subset=target_attr)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test data split (already done in previous phases)\n",
    "\n",
    "`wine_train_df` and `wine_test_df` were already created back in phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_train_df shape: (3918, 12)\n",
      "wine_test_df shape: (980, 12)\n"
     ]
    }
   ],
   "source": [
    "wine_train_df = pd.read_csv('data/wine_train_df.csv', sep=\",\")\n",
    "wine_test_df = pd.read_csv('data/wine_test_df.csv', sep=\",\")\n",
    "print(f\"wine_train_df shape: {wine_train_df.shape}\")\n",
    "print(f\"wine_test_df shape: {wine_test_df.shape}\")\n",
    "del wine_train_df\n",
    "del wine_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation split\n",
    "\n",
    "This step will be omitted because cross-validation will be used to select models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checking out attribute types\n",
    "Already completed in previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identifying attributes with missingness above threshold\n",
    "Already completed in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict = sml_utils.get_missingness(train_cap_x_df, missingness_threshold)\n",
    "missingness_drop_list = return_dict['missingness_drop_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identifying non machine learning attributes\n",
    "Already completed in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ml_attr_list = [] # no non-machine learning attributes were identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Looking at attributes to exclude from ML\n",
    "Already completed in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_attr_drop_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Establishing ML attribute configuration\n",
    "Already completed in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ignore_list = missingness_drop_list + non_ml_attr_list + ml_attr_drop_list\n",
    "ml_ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the remaining numerical attributes to be used\n",
    "\n",
    "numerical_attr = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',]\n",
    "\n",
    "# Identifying the remaining nominal attributes to be used\n",
    "\n",
    "nominal_attr = [] \n",
    "\n",
    "assert(train_cap_x_df.shape[1] == len(ml_ignore_list) + len(nominal_attr) + len(numerical_attr))\n",
    "\n",
    "print(f'ml_ignore_list: {ml_ignore_list}')\n",
    "print(f'\\nnumerical_attr: {numerical_attr}')\n",
    "print(f'nominal_attr: {nominal_attr}')\n",
    "\n",
    "print(f'\\nnumber of machine learning attributes: {len(numerical_attr) + len(nominal_attr)}')\n",
    "print(f'\\nnumerical_attr and nominal_attr: {numerical_attr + nominal_attr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building the composite estimators\n",
    "Shortlist already created in `model_exp_x.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_dict = {\n",
    "    'LinearRegression': LinearRegression(\n",
    "        fit_intercept=True, \n",
    "        copy_X=True, \n",
    "        n_jobs=None, \n",
    "        positive=False\n",
    "    ),\n",
    "\n",
    "    'ElasticNet': ElasticNet(\n",
    "        fit_intercept=True,\n",
    "        copy_X=True, \n",
    "        positive=False, \n",
    "\n",
    "        alpha=1.0,  \n",
    "        l1_ratio=0.5,\n",
    "        max_iter=max_iter, \n",
    "        tol=0.0001, \n",
    "        selection='cyclic',\n",
    "        \n",
    "        precompute=False, \n",
    "        warm_start=False, \n",
    "        random_state=elastic_net_random_state, \n",
    "    ),\n",
    "\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(\n",
    "        criterion='squared_error', \n",
    "        splitter='best', \n",
    "        max_depth=None, \n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        min_weight_fraction_leaf=0.0, \n",
    "        max_features=None, \n",
    "        random_state=decision_tree_random_state, \n",
    "        max_leaf_nodes=None, \n",
    "        min_impurity_decrease=0.0, \n",
    "        ccp_alpha=0.0, \n",
    "        monotonic_cst=None\n",
    "    ),\n",
    "\n",
    "    'RandomForestRegressor': RandomForestRegressor(\n",
    "        \n",
    "        n_estimators=100,\n",
    "        bootstrap=True, \n",
    "        oob_score=False, \n",
    "        n_jobs=None,\n",
    "        verbose=0, \n",
    "        warm_start=False,\n",
    "        max_samples=None, \n",
    "        \n",
    "        criterion='squared_error',  # in DecisionTreeRegressor\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=1.0,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        random_state=random_forest_random_state,\n",
    "        ccp_alpha=0.0,\n",
    "        monotonic_cst=None\n",
    "    ),\n",
    "\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(\n",
    "        \n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1,\n",
    "        validation_fraction=0.1,  # proportion of training data to set aside as validation set for early stopping\n",
    "        n_iter_no_change=None,  # used to decide if early stopping will be used to terminate training\n",
    "        tol=0.0001,  # tolerance for early stopping\n",
    "        subsample=1.0,  # fraction of samples to be used for fitting the individual base learners\n",
    "        criterion='friedman_mse',  # what measures quality of split\n",
    "\n",
    "        loss='squared_error',  # in DecisionTreeRegressor\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=None,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        random_state=gradient_boosting_random_state,\n",
    "        ccp_alpha=0.0,\n",
    "        \n",
    "        init=None, \n",
    "        alpha=0.9, \n",
    "        verbose=0, \n",
    "        warm_start=False\n",
    "        \n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing pipeline (already done in `model_exp_x.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "# discretizer not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used as there are no nominal attributes\n",
    "nominal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "        (\"ohe\", OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('nominal', nominal_transformer, nominal_attr),\n",
    "            ('numerical', numerical_transformer, numerical_attr)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_estimator = Pipeline(steps=[('preprocessor', preprocessor), ('estimator', estimator_dict)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output of preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numerical__fixed acidity</th>\n",
       "      <th>numerical__volatile acidity</th>\n",
       "      <th>numerical__citric acid</th>\n",
       "      <th>numerical__residual sugar</th>\n",
       "      <th>numerical__chlorides</th>\n",
       "      <th>numerical__free sulfur dioxide</th>\n",
       "      <th>numerical__total sulfur dioxide</th>\n",
       "      <th>numerical__density</th>\n",
       "      <th>numerical__pH</th>\n",
       "      <th>numerical__sulphates</th>\n",
       "      <th>numerical__alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515119</td>\n",
       "      <td>-1.076233</td>\n",
       "      <td>0.227731</td>\n",
       "      <td>0.340419</td>\n",
       "      <td>-0.813688</td>\n",
       "      <td>0.534065</td>\n",
       "      <td>-0.641932</td>\n",
       "      <td>-0.447041</td>\n",
       "      <td>-0.328261</td>\n",
       "      <td>-0.702445</td>\n",
       "      <td>1.540371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.669188</td>\n",
       "      <td>-0.288777</td>\n",
       "      <td>0.895832</td>\n",
       "      <td>1.002071</td>\n",
       "      <td>-0.217212</td>\n",
       "      <td>0.773947</td>\n",
       "      <td>1.355106</td>\n",
       "      <td>0.903370</td>\n",
       "      <td>-0.061886</td>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.821712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.498203</td>\n",
       "      <td>0.400248</td>\n",
       "      <td>-0.022807</td>\n",
       "      <td>0.184737</td>\n",
       "      <td>-0.400743</td>\n",
       "      <td>-0.605377</td>\n",
       "      <td>-1.022320</td>\n",
       "      <td>-0.460280</td>\n",
       "      <td>0.404271</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.481506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041396</td>\n",
       "      <td>-0.879369</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>-0.924503</td>\n",
       "      <td>-0.446626</td>\n",
       "      <td>-0.125612</td>\n",
       "      <td>-0.879675</td>\n",
       "      <td>-0.304718</td>\n",
       "      <td>0.137896</td>\n",
       "      <td>0.442168</td>\n",
       "      <td>0.237153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988842</td>\n",
       "      <td>0.203384</td>\n",
       "      <td>-0.607396</td>\n",
       "      <td>2.432407</td>\n",
       "      <td>0.333383</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.855846</td>\n",
       "      <td>1.883079</td>\n",
       "      <td>0.071302</td>\n",
       "      <td>0.089980</td>\n",
       "      <td>-0.088652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numerical__fixed acidity  numerical__volatile acidity  \\\n",
       "0                  0.515119                    -1.076233   \n",
       "1                 -0.669188                    -0.288777   \n",
       "2                 -1.498203                     0.400248   \n",
       "3                  0.041396                    -0.879369   \n",
       "4                  0.988842                     0.203384   \n",
       "\n",
       "   numerical__citric acid  numerical__residual sugar  numerical__chlorides  \\\n",
       "0                0.227731                   0.340419             -0.813688   \n",
       "1                0.895832                   1.002071             -0.217212   \n",
       "2               -0.022807                   0.184737             -0.400743   \n",
       "3                0.144218                  -0.924503             -0.446626   \n",
       "4               -0.607396                   2.432407              0.333383   \n",
       "\n",
       "   numerical__free sulfur dioxide  numerical__total sulfur dioxide  \\\n",
       "0                        0.534065                        -0.641932   \n",
       "1                        0.773947                         1.355106   \n",
       "2                       -0.605377                        -1.022320   \n",
       "3                       -0.125612                        -0.879675   \n",
       "4                        0.054300                         0.855846   \n",
       "\n",
       "   numerical__density  numerical__pH  numerical__sulphates  numerical__alcohol  \n",
       "0           -0.447041      -0.328261             -0.702445            1.540371  \n",
       "1            0.903370      -0.061886              0.266074           -0.821712  \n",
       "2           -0.460280       0.404271              0.001933            0.481506  \n",
       "3           -0.304718       0.137896              0.442168            0.237153  \n",
       "4            1.883079       0.071302              0.089980           -0.088652  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trans_train_cap_x_df = pd.DataFrame(\n",
    "    data=composite_estimator[0].fit_transform(train_cap_x_df),\n",
    "    index=train_cap_x_df.index,\n",
    "    columns=[attr_name for attr_name in composite_estimator[0].get_feature_names_out()]\n",
    ")\n",
    "trans_train_cap_x_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fitting/evaluating composite estimators\n",
    "### Surveying the candidate default models by fitting them on the whole train set\n",
    "This was already completed in `model_exp_x.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "ElasticNet\n",
      "DecisionTreeRegressor\n",
      "RandomForestRegressor\n",
      "GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "return_dict = sml_utils.model_survey_fit(preprocessor, estimator_dict, train_cap_x_df, train_y_df)\n",
    "trained_estimator_dict = return_dict['trained_estimator_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimating the test error rate using k-fold cv (using KFold splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximal control k-fold cross validation splitter\n",
    "splitter = KFold(n_splits=kfold_n_splits, \n",
    "                 shuffle=kfold_shuffle, \n",
    "                 random_state=kfold_random_state\n",
    ")\n",
    "\n",
    "# perform cross validation on models\n",
    "sml_utils.model_survey_cross_val_and_analysis(preprocessor, estimator_dict, train_cap_x_df, train_y_df, scoring, splitter, target_attr, trained_estimator_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Shortlisting default models\n",
    "The shortlist was chosen in `model_exp_x.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del estimator_dict['INSERT CHOSEN TO DELETE HERE']\n",
    "\n",
    "# estimator_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Tuning hyperparameters of shortlisted models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy logspace function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.logspace(-1.1, -0.7, num=5)) # ADJUST THIS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy arange function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.arange(0.7, 1.0, step=0.1).round(2)) # ADJUST THIS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up hyperparameter space for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_param_grid = {\n",
    "    'preprocessor__numerical__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__nominal__target_encoder__smooth': ['auto']\n",
    "}\n",
    "\n",
    "elastic_net_alpha_log_space_min = -1.1\n",
    "elastic_net_alpha_log_space_max = -0.7\n",
    "elastic_net_alpha_log_space_num = 5\n",
    "elastic_net_l1_ratio_log_space_min = 0.7  # l1_ratio = 0 gives Ridge\n",
    "elastic_net_l1_ratio_log_space_max = 1.0  # l1_ratio = 1 gives Lasso\n",
    "elastic_net_l1_ratio_log_space_step = 0.1\n",
    "elastic_net_param_grid = preproc_param_grid | {\n",
    "    'estimator__alpha': list(np.logspace(\n",
    "        elastic_net_alpha_log_space_min, \n",
    "        elastic_net_alpha_log_space_max, \n",
    "        num=elastic_net_alpha_log_space_num)),\n",
    "    'estimator__l1_ratio': list(np.arange(\n",
    "        elastic_net_l1_ratio_log_space_min, \n",
    "        elastic_net_l1_ratio_log_space_max, \n",
    "        step=elastic_net_l1_ratio_log_space_step).round(2))\n",
    "}\n",
    "\n",
    "linear_regression_param_grid = preproc_param_grid | {\n",
    "    'estimator__fit_intercept': [True],  # default value True\n",
    "    'estimator__copy_X': [True],  # default value True\n",
    "    'estimator__n_jobs': [None], # default value None\n",
    "    'estimator__positive': [False]  # default value False\n",
    "}\n",
    "\n",
    "decision_tree_param_grid = preproc_param_grid | {\n",
    "    'estimator__max_depth': [None], # default value None\n",
    "    'estimator__min_samples_split': [2], # default value 2\n",
    "    'estimator__min_samples_leaf': [1], # default value 1\n",
    "    'estimator__max_features': [None], # default value None\n",
    "    'estimator__max_leaf_nodes': [None] # default value None\n",
    "}\n",
    "\n",
    "random_forest_param_grid = preproc_param_grid | {\n",
    "    \n",
    "    'estimator__n_estimators': [100],  # default 100\n",
    "    'estimator__bootstrap': [True],  # default value True\n",
    "    'estimator__max_samples': [None], # default value None\n",
    "    \n",
    "    'estimator__criterion': ['squared_error'],  # default value 'squared_error' for DecisionTreeRegressor\n",
    "    'estimator__max_depth': [None],  # default value None\n",
    "    'estimator__min_samples_split': [2],  # default value 2\n",
    "    'estimator__min_samples_leaf': [1], # default value 1\n",
    "    'estimator__min_weight_fraction_leaf': [0.0],  # default value 0.0\n",
    "    'estimator__max_features': [None],  # default value None\n",
    "    'estimator__max_leaf_nodes': [None],  # default value None\n",
    "    'estimator__min_impurity_decrease': [0.0],  # default value 0.0\n",
    "    'estimator__ccp_alpha': [0.0],  # default value 0.0\n",
    "    'estimator__monotonic_cst': [None]  # default value None\n",
    "    \n",
    "}\n",
    "\n",
    "gradient_boosting_param_grid = preproc_param_grid | {\n",
    "\n",
    "    'estimator__n_estimators': [100],  # default value 100\n",
    "\n",
    "    # early stopping\n",
    "    'estimator__n_iter_no_change': [None],  # default value None\n",
    "    'estimator__tol': [0.0001],  # default value 0.0001\n",
    "    'estimator__validation_fraction': [0.1],  # default value 0.1\n",
    "\n",
    "    # regularization\n",
    "    'estimator__learning_rate': [0.1],  # default value 0.1\n",
    "    'estimator__max_depth': [3],  # default value 3\n",
    "    'estimator__subsample': [1.0],  # default value 1.0\n",
    "    'estimator__max_features': [None]  # default value None\n",
    "    \n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LinearRegression': linear_regression_param_grid,\n",
    "    'ElasticNet': elastic_net_param_grid,\n",
    "    'DecisionTreeRegressor': decision_tree_param_grid,\n",
    "    'RandomForestRegressor': random_forest_param_grid,\n",
    "    'GradientBoostingRegressor': gradient_boosting_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximal control k-fold cross validation splitter\n",
    "splitter = KFold(n_splits=gs_cv_kfold_n_splits, \n",
    "                 shuffle=gs_cv_kfold_shuffle, \n",
    "                 random_state=gs_cv_kfold_random_state\n",
    ")\n",
    "\n",
    "# collect grid seach cv results here\n",
    "tuned_estimator_dict = {}\n",
    "\n",
    "for estimator_name, estimator in estimator_dict.items():\n",
    "    \n",
    "    print(estimator_name)\n",
    "\n",
    "    composite_estimator = \\\n",
    "    Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('estimator', estimator)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    grid_search_cv = GridSearchCV(\n",
    "        estimator=composite_estimator, \n",
    "        param_grid=param_grids[estimator_name], \n",
    "        scoring=scoring,\n",
    "        n_jobs=None, \n",
    "        refit=scoring[0],\n",
    "        cv=splitter, \n",
    "        verbose=0, \n",
    "        pre_dispatch='2*n_jobs', \n",
    "        error_score=np.nan, \n",
    "        return_train_score=True\n",
    "    )\n",
    "    grid_search_cv.fit(train_cap_x_df, train_y_df.values.ravel())\n",
    "\n",
    "    tuned_estimator_dict[estimator_name] = grid_search_cv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flexibility plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_survey_results_df = pd.DataFrame()\n",
    "for estimator_name, grid_search_cv in tuned_estimator_dict.items():\n",
    "    print('\\n\\n')\n",
    "    return_dict = sml_utils.plot_flexibility(\n",
    "        grid_search_cv=grid_search_cv,\n",
    "        estimator_name=estimator_name,\n",
    "        scoring=scoring\n",
    "    )\n",
    "    results_df = return_dict['results_df']\n",
    "    results_df['estimator_name'] = estimator_name\n",
    "    results_df = results_df[['estimator_name'] + [attr for attr in results_df if attr not in ['estimator_name']]]\n",
    "    gs_survey_results_df = pd.concat([gs_survey_results_df, results_df], axis=0)\n",
    "\n",
    "gs_survey_results_df = gs_survey_results_df.sort_values(['score', 'best_test_score'])\n",
    "gs_survey_results_df = gs_survey_results_df.reset_index(drop=True)\n",
    "gs_survey_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using cross validation for model selection (TRY ENSEMBLE METHODS??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximal control k-fold cross validation splitter\n",
    "splitter = KFold(n_splits=kfold_n_splits, \n",
    "                 shuffle=kfold_shuffle, \n",
    "                 random_state=kfold_random_state\n",
    ")\n",
    "\n",
    "# perform cross validation on models\n",
    "sml_utils.model_tuning_cross_val_and_analysis(tuned_estimator_dict, train_cap_x_df, train_y_df, scoring, splitter, target_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best hyperparameters for each estimator\\n')\n",
    "\n",
    "for index, row in gs_survey_results_df.iterrows():\n",
    "    \n",
    "    print(f'\\nestimator_name: {row['estimator_name']}; score: {row['score']}')\n",
    "    \n",
    "    param_grids_ = param_grids[row['estimator_name']]\n",
    "    for hyperparameter_name, hyperparameter_value in row['grid_search_cv'].best_params_.items():\n",
    "\n",
    "        if len(param_grids_[hyperparameter_name]) > 1 and not show_all_params:  #  and only_show_searched_params:  # only check the hyperparameter you are varing\n",
    "            print(f'   hyperparameter_name: {hyperparameter_name}; hyperparameter_value: {hyperparameter_value}')\n",
    "        elif show_all_params:\n",
    "            print(f'   hyperparameter_name: {hyperparameter_name}; hyperparameter_value: {hyperparameter_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min test score from GridSeachCV\n",
    "min_test_score = gs_survey_results_df.best_test_score.min()\n",
    "print(f'\\nMinimum test score from GridSearchCV: {min_test_score}')\n",
    "\n",
    "# name of estimator with the min test score\n",
    "estimator_name = gs_survey_results_df.loc[gs_survey_results_df['best_test_score'] == min_test_score, 'estimator_name'].iloc[0]\n",
    "print(f'\\nThe estimator with minimum test score from GridSearchCV is considered the best model. It is: {estimator_name}')\n",
    "\n",
    "# best estimator\n",
    "best_model = gs_survey_results_df.loc[gs_survey_results_df['best_test_score'] == min_test_score, 'grid_search_cv'].values[0].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluating the tuned composite estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_best_model_on_train_set = root_mean_squared_error(train_y_df, best_model.predict(train_cap_x_df))\n",
    "rmse_best_model_on_train_set\n",
    "print(f'best_model is the trained estimator that performed the best in GridSearchCV.\\n'\n",
    "      f'\\nIt was trained on the whole train set using the hyperparameter combination\\n'\n",
    "      f'that gave the lowest estimate of test error rate in cross validation.\\n'\n",
    "      f'\\nThe rmse of the best_model when prediction on the whole train set is {rmse_best_model_on_train_set}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Check for false discoveries\n",
    "Shuffling the target and using cross validation to see if the discoveries are real or just random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sml_utils.check_for_false_discoveries(tuned_estimator_dict, train_cap_x_df, train_y_df, scoring, splitter, target_attr, shuffle_target=True,\n",
    "                                      shuffle_target_random_state=42, gs_survey_results_df=gs_survey_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Selecting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_estimator = None  # choose best_model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Evaluating generalization/making predictions on the test set with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if composite_estimator is None:\n",
    "    sys.exit('no model was selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/wine_test_df.csv', index_col='index')\n",
    "test_df.index.name = None\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cap_x_df = test_df.iloc[:, :-1]\n",
    "test_cap_x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_df = test_df.iloc[:, -1].to_frame()\n",
    "test_y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = root_mean_squared_error(test_y_df, composite_estimator.predict(test_cap_x_df))\n",
    "print(f'test_rmse: {test_rmse}')\n",
    "print(f'relative test_rmse: {test_rmse/np.mean(test_y_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_df = composite_estimator.predict(test_cap_x_df)\n",
    "data_set_name = 'test'\n",
    "reg_utils.plot_pred_vs_actual(pred_y_df, test_y_df, data_set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'script run time: {(end - start)/60} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_usml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
